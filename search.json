[
  {
    "objectID": "setup/setup_instructions.html",
    "href": "setup/setup_instructions.html",
    "title": "Repository Setup Instructions",
    "section": "",
    "text": "This repository uses a Python environment similar to the EMIT Data Resources Repository, with the addition of the scikit-image library. If you have previously installed the emit_tutorials environment, you can simply add this library and use that environment. If you plan to work with the EMIT-Data-Resources Repository, this lpdaac_vitals environment can be used.\n\nThe how-tos and tutorials in this repository require a NASA Earthdata account, an installation of Git, and a compatible Python Environment. We recommend mamba to manage Python packages, but if you are already using another package manager like conda, that will work as well. To install mamba, download mambaforge for your operating system. If using Windows, be sure to check the box to “Add mamba to my PATH environment variable” to enable use of mamba directly from your command line interface. Note that this may cause an issue if you have an existing mamba install through Anaconda.\n\n\nThese Python Environments will work for all of the guides, how-to’s, and tutorials within this repository. A .yml file that can be used to set up the necessary environment has been included in the repository for both Windows and MacOS. Use the appropriate file in the steps below.\n\nIf you wish to use conda as your package manager you can simply substitute ‘conda’ for ‘mamba’ in the steps below.\n\n\nUsing your preferred command line interface (command prompt, terminal, cmder, etc.) navigate to your local copy of the repository, then type the following to create a compatible Python environment.\nFor Windows:\nmamba env create -f setup/lpdaac_vitals_windows.yml\nFor MacOS(tested on arm-64):\nmamba env create -f setup/lpdaac_vitals_macos.yml\nNext, activate the Python Environment that you just created.\nmamba activate lpdaac_vitals \nNow you can launch Jupyter Notebook to open the notebooks included.\njupyter notebook \n\nIf you’re having trouble creating a compatible Python Environment or having an issue with one of the above environments, you can also try to create one using the commands below. Using your preferred command line interface (command prompt, terminal, cmder, etc.) type the following to create a compatible Python environment.\nFor Windows:\nmamba create -n lpdaac_vitals -c conda-forge --yes python=3.11 fiona=1.8.22 gdal hvplot geoviews rioxarray rasterio jupyter geopandas earthaccess jupyter_bokeh h5py h5netcdf spectral scikit-image\nFor MacOSX:\nmamba create -n lpdaac_vitals -c conda-forge --yes python=3.9 gdal=3.6.4 hvplot geoviews rioxarray rasterio geopandas fiona=1.8.22 jupyter earthaccess jupyter_bokeh h5py h5netcdf spectral scikit-image\nAfter this, you should be able to do steps 2 and 3 above.\nStill having trouble getting a compatible Python environment set up? Contact LP DAAC User Services.\n\n\n\n\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 09-29-2023\n¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I."
  },
  {
    "objectID": "setup/setup_instructions.html#python-environment-setup",
    "href": "setup/setup_instructions.html#python-environment-setup",
    "title": "Repository Setup Instructions",
    "section": "",
    "text": "These Python Environments will work for all of the guides, how-to’s, and tutorials within this repository. A .yml file that can be used to set up the necessary environment has been included in the repository for both Windows and MacOS. Use the appropriate file in the steps below.\n\nIf you wish to use conda as your package manager you can simply substitute ‘conda’ for ‘mamba’ in the steps below.\n\n\nUsing your preferred command line interface (command prompt, terminal, cmder, etc.) navigate to your local copy of the repository, then type the following to create a compatible Python environment.\nFor Windows:\nmamba env create -f setup/lpdaac_vitals_windows.yml\nFor MacOS(tested on arm-64):\nmamba env create -f setup/lpdaac_vitals_macos.yml\nNext, activate the Python Environment that you just created.\nmamba activate lpdaac_vitals \nNow you can launch Jupyter Notebook to open the notebooks included.\njupyter notebook \n\nIf you’re having trouble creating a compatible Python Environment or having an issue with one of the above environments, you can also try to create one using the commands below. Using your preferred command line interface (command prompt, terminal, cmder, etc.) type the following to create a compatible Python environment.\nFor Windows:\nmamba create -n lpdaac_vitals -c conda-forge --yes python=3.11 fiona=1.8.22 gdal hvplot geoviews rioxarray rasterio jupyter geopandas earthaccess jupyter_bokeh h5py h5netcdf spectral scikit-image\nFor MacOSX:\nmamba create -n lpdaac_vitals -c conda-forge --yes python=3.9 gdal=3.6.4 hvplot geoviews rioxarray rasterio geopandas fiona=1.8.22 jupyter earthaccess jupyter_bokeh h5py h5netcdf spectral scikit-image\nAfter this, you should be able to do steps 2 and 3 above.\nStill having trouble getting a compatible Python environment set up? Contact LP DAAC User Services."
  },
  {
    "objectID": "setup/setup_instructions.html#contact-info",
    "href": "setup/setup_instructions.html#contact-info",
    "title": "Repository Setup Instructions",
    "section": "",
    "text": "Email: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 09-29-2023\n¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I."
  },
  {
    "objectID": "python/ECOSTRESS-EMIT_Carpinteria_Workshop.html",
    "href": "python/ECOSTRESS-EMIT_Carpinteria_Workshop.html",
    "title": "ECOSTRESS-EMIT Workshop Fall 2023 - Carpinteria Salt Marsh Analysis",
    "section": "",
    "text": "Gregory Halverson Jet Propulsion Laboratory, California Institute of Technology\nClaire Villanueva-Weeks Jet Propulsion Laboratory, California Institute of Technology\nThis research was carried out at the Jet Propulsion Laboratory, California Institute of Technology, and was sponsored by ECOSTRESS and the National Aeronautics and Space Administration (80NM0018D0004).\n© 2023. All rights reserved.\n## Summary In this notebook we will open an EMIT L2A Reflectance product file in NETCDF4 format and an ECOSTRESS L2 Land Surface Temperature product file in GEOTIFF format. We will demonstrate using specific bands to calculate NDVI and use holoviews to plot the EMIT spectra and calculated NDVI. We will also demonstrate opening and mapping the ECOSTRESS L2 Land Surface Temperature product."
  },
  {
    "objectID": "python/ECOSTRESS-EMIT_Carpinteria_Workshop.html#importing-libraries",
    "href": "python/ECOSTRESS-EMIT_Carpinteria_Workshop.html#importing-libraries",
    "title": "ECOSTRESS-EMIT Workshop Fall 2023 - Carpinteria Salt Marsh Analysis",
    "section": "Importing Libraries",
    "text": "Importing Libraries\nThese are some built-in Python functions we need for this notebook, including functions for handling filenames and dates.\n\n# !mamba install -q -y cartopy earthaccess geoviews rasterstats folium geopandas hvplot holoviews matplotlib netCDF4 numpy pandas rasterio rasterstats rioxarray seaborn scikit-image shapely xarray\n\n\nimport os, sys\nfrom os.path import join, abspath, basename, splitext\nfrom glob import glob\nfrom datetime import datetime, date, timedelta\nfrom zipfile import ZipFile\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nWe’re using the rioxarray package for loading raster data from a GeoTIFF file, and we’re importing it as rxr. We’re using the numpy library to handle arrays, and we’re importing it as np. We’re using the rasterio package to subset the data.\n\nimport rioxarray as rxr\nimport numpy as np\nimport rasterio as rio\n\nWe’re using the geopandas library to load vector data from GeoJSON files, and we’re importing it as gpd. We’re using the shapely library to handle vector data and the pyproj library to handle projections.\n\nimport geopandas as gpd\nfrom shapely.geometry import Point, box\nfrom shapely.ops import transform\nfrom pyproj import Transformer\n\nWe’re using the pandas library to handle tables, and we’re importing it as pd.\n\nimport pandas as pd\n\nWe’re using the seaborn library to produce our graphs, and we’re importing it as sns. We’re using the hvplot library to produce our maps.\n\nimport seaborn as sns\nimport hvplot.xarray\nimport hvplot.pandas\n\n\nimport earthaccess\nfrom osgeo import gdal\nimport math\nimport netCDF4 as nc\n\nImport the emit_tools module and call use from emit_tools import emit_xarray help(emit_xarray) the help function to see how it can be used.\n\nNote: This function currently works with L1B Radiance and L2A Reflectance Data.\n\n\nsys.path.append('../modules/')\nfrom emit_tools import emit_xarray\nhelp(emit_xarray)"
  },
  {
    "objectID": "python/ECOSTRESS-EMIT_Carpinteria_Workshop.html#defining-constants",
    "href": "python/ECOSTRESS-EMIT_Carpinteria_Workshop.html#defining-constants",
    "title": "ECOSTRESS-EMIT Workshop Fall 2023 - Carpinteria Salt Marsh Analysis",
    "section": "Defining Constants",
    "text": "Defining Constants\nThese constants define the dimensions of our figures. Feel free to adjust these to fit your display.\n\nFIG_WIDTH_PX = 1080\nFIG_HEIGHT_PX = 720\nFIG_WIDTH_IN = 16\nFIG_HEIGHT_IN = 9\nFIG_ALPHA = 0.7\nBASEMAP = \"ESRI\"\nSEABORN_STYLE = \"whitegrid\"\nsns.set_style(SEABORN_STYLE)\n\nThis is the location of the ECOSTRESS and EMIT product files\n\nDATA_DIRECTORY = \"/home/jovyan/shared/ECOSTRESS-EMIT_data/data/\" # FIXME set this to the common path in OpenScapes \nprint(f\"data directory: {DATA_DIRECTORY}\")"
  },
  {
    "objectID": "python/ECOSTRESS-EMIT_Carpinteria_Workshop.html#loading-and-mapping-an-ecostress-lst-granule",
    "href": "python/ECOSTRESS-EMIT_Carpinteria_Workshop.html#loading-and-mapping-an-ecostress-lst-granule",
    "title": "ECOSTRESS-EMIT Workshop Fall 2023 - Carpinteria Salt Marsh Analysis",
    "section": "Loading and Mapping an ECOSTRESS LST granule",
    "text": "Loading and Mapping an ECOSTRESS LST granule\nFirst, let’s trying opening a data layer from a product file.\n\nECOSTRESS_fp = join(DATA_DIRECTORY, \"ECOv002_L2T_LSTE_26921_001_11SKU_20230405T190258_0710_01_LST.tif\")\nECOSTRESS_fp\n\nWe’re using rioxarray to open the surface temperature product on the 11SKU tile covering the Carpinteria Salt Marsh. We’re passing the filename of the GeoTIFF file directly into rioxarray.\n\nLST_K_raster = rxr.open_rasterio(ECOSTRESS_fp).squeeze('band', drop=True)\nLST_K_raster\n\nThe hvplot package extends xarray to allow us to plot maps. We’re reprojecting the raster geographic projection EPSG 4326 to overlay on the basemap with a latitude and longitude graticule. We’re using the jet color scheme to render temperature with a rainbow of colors with red meaning hot and blue meaning cool. We’re setting the alpha to make the raster semi-transparent on top of the basemap. We’re filtering out values lower than the 2% percentile and higher than the 98% percentile to make the variation in the image more visible.\nThe temperatures in the L2T_LSTE product are given in Kelvin. To convert them to Celsius, we subtract 273.15.\n\nLST_C_raster = LST_K_raster.copy()\nLST_C_raster.data = LST_K_raster.data - 273.15\n\nLST_C_map = LST_C_raster.rio.reproject(\"EPSG:4326\").hvplot.image(\n    geo=True,\n    cmap=\"jet\",\n    tiles=BASEMAP,\n    alpha=FIG_ALPHA,\n    width=FIG_WIDTH_PX,\n    height=FIG_HEIGHT_PX,\n    clim=(LST_C_raster.quantile(0.02), LST_C_raster.quantile(0.98)),\n    title=f\"{splitext(basename(ECOSTRESS_fp))[0]} Surface Temperature (Celsius)\"\n)\n\nLST_C_map = LST_C_map.options(xlabel=\"Longitude\", ylabel=\"Latitude\")\nLST_C_map"
  },
  {
    "objectID": "python/ECOSTRESS-EMIT_Carpinteria_Workshop.html#loading-an-emit-reflectance-granule",
    "href": "python/ECOSTRESS-EMIT_Carpinteria_Workshop.html#loading-an-emit-reflectance-granule",
    "title": "ECOSTRESS-EMIT Workshop Fall 2023 - Carpinteria Salt Marsh Analysis",
    "section": "Loading an EMIT Reflectance Granule",
    "text": "Loading an EMIT Reflectance Granule\nSo now that weve opened up and visualized an ECOSTRESS collection 2 LST granule, lets try opening a data layer from the EMIT product file.\nEMIT L2A Reflectance Data are distributed in a non-orthocorrected spatially raw NetCDF4 (.nc) format. .nc files\nTo open up the .nc file we will use the netCDF4, xarray and emit_tools libraries.\n\nEMIT_fp = join(DATA_DIRECTORY, \"EMIT_L2A_RFL_001_20230405T190323_2309513_003.nc\")\nEMIT_fp\n\nOpening the file with nc allows us to see file information and the different groups, theres reflectance which were concerned with, sensor band parameters, and location.\n\nds_nc = nc.Dataset(EMIT_fp)\nds_nc\n\nNow we will use the emit_xarray function from the emit_tools module wirh ortho set to True to orthorectify the L2A reflectance data and place it into an xarray.Dataset.\n\nFor a detailed walkthrough of the orthorectification process using the GLT see section 2 of the How_to_Orthorectify.ipynb in the how-tos folder.\n\n\nds = emit_xarray(EMIT_fp,ortho=True)\nds"
  },
  {
    "objectID": "python/ECOSTRESS-EMIT_Carpinteria_Workshop.html#visualizing-emit-spectral-and-spatial-data",
    "href": "python/ECOSTRESS-EMIT_Carpinteria_Workshop.html#visualizing-emit-spectral-and-spatial-data",
    "title": "ECOSTRESS-EMIT Workshop Fall 2023 - Carpinteria Salt Marsh Analysis",
    "section": "Visualizing EMIT Spectral and Spatial Data",
    "text": "Visualizing EMIT Spectral and Spatial Data\nHere we picked out and mapped wavelengths nearest to 800 nm and 675 nm using the .sel function from xarray\n\nds.sel(wavelengths=800, method='nearest').hvplot.image(x='longitude', y='latitude',tiles=BASEMAP,cmap='viridis', aspect = 'equal', frame_width=400,title=f\"{splitext(basename(EMIT_fp))[0]} ~800 nm\") +\\\n    ds.sel(wavelengths=675, method='nearest').hvplot.image(x='longitude', y='latitude',tiles=BASEMAP,cmap='viridis', aspect = 'equal', frame_width=400,title=f\"{splitext(basename(EMIT_fp))[0]} ~675 nm\")\n\nThese are the nearest to 800 nm and 675 nm wavelengths, they can be used to calculate the NDVI using a ratio of the difference between between the wavelengths to the sum of the wavelengths.NDVI is a metric by which we can estimate vegetation greenness.\n\nNDVI = (ds.sel(wavelengths=800, method='nearest') - ds.sel(wavelengths=675, method='nearest'))/(ds.sel(wavelengths=800, method='nearest') + ds.sel(wavelengths=675, method='nearest'))\nNDVI.hvplot.image(\n    x='longitude', \n    y='latitude',\n    width=FIG_WIDTH_PX,\n    height=FIG_HEIGHT_PX,\n    tiles=BASEMAP,\n    cmap='RdYlGn', \n    aspect = 'equal', \n    title=f\"{splitext(basename(EMIT_fp))[0]} Vegetation Index\")"
  },
  {
    "objectID": "python/ECOSTRESS-EMIT_Carpinteria_Workshop.html#isolating-our-region-of-interest",
    "href": "python/ECOSTRESS-EMIT_Carpinteria_Workshop.html#isolating-our-region-of-interest",
    "title": "ECOSTRESS-EMIT Workshop Fall 2023 - Carpinteria Salt Marsh Analysis",
    "section": "Isolating Our Region of Interest",
    "text": "Isolating Our Region of Interest\nOur ROI is the Carpinteria Salt Marsh Habitat, which is a marsh reserve here in Southern California that is home to many different species of plants and animals (more information- https://carpinteria.ucnrs.org/). Tomorrow when we take a fieldtrip there where there will be a guided tour where we will be learning more about the Carpinteria Salt Marsh and its ecology.\nTo clip the raster image to the extent of the vector dataset, we want to subset the raster to the bounds of the vector dataset. This dataset is included here in GeoJSON format, which we’ll load in as a geodatagrame using the geopandas package.\n\nlandcoverfile = join(DATA_DIRECTORY,\"landcover.geojson\")\nlandcover_latlon = gpd.read_file(landcoverfile)\n\nlandcover_latlon\n\nTo align this vector dataset with the raster datasets, we need to project it to the UTM projection used for the ECOSTRESS rasters.\n\nLST_raster = LST_K_raster.copy()\nlandcover_UTM = landcover_latlon.to_crs(LST_raster.rio.crs)\n\nlandcover_UTM\n\nThis vector dataset contains polygons classifying the surface of the Carpinteria Salt Marsh into channel, salt flat, upland, pan, and marsh. You can see that this vector dataset contains 5 polygons that classify the Carpinteria Salt Marhs the Marsh into channel, salt flat, upland, pan, and marsh.\n\nlandcover_colors = {\n    \"channel\": \"blue\",\n    \"marsh\": \"yellow\",\n    \"pan\": \"green\",\n    \"salt flat\": \"white\",\n    \"upland\": \"brown\"\n}\n\nlandcover_map = landcover_latlon.to_crs(\"EPSG:4326\").hvplot.polygons(\n    geo=True,\n    color=landcover_UTM[\"type\"].apply(lambda type: landcover_colors[type]),\n    tiles=BASEMAP,\n    alpha=FIG_ALPHA,\n    width=FIG_WIDTH_PX,\n    height=FIG_HEIGHT_PX,\n    title=\"Carpinteria Salt Marsh Habitat Polygons\"\n)\n\nlandcover_map = landcover_map.options(xlabel=\"Longitude\", ylabel=\"Latitude\")\nlandcover_map\n\nNow we can use the clip function from rasterio to mask out a subset of the the LST and NDVI datasets to the extent of the polygons from the vector dataset. Setting all_touched to True will include pixels that intersect with the edges of the polygons.\n\nLST_clip = LST_raster.rio.clip(landcover_latlon.geometry.values,landcover_latlon.crs, all_touched=True)\nLST_map = LST_clip.hvplot.image(\n    cmap='jet',\n    alpha=FIG_ALPHA,\n    width=FIG_WIDTH_PX,\n    height=FIG_HEIGHT_PX,\n    title = \"Carpinteria Salt Marsh Surface Temperature (Celsius)\"\n) * landcover_UTM.hvplot(fill_color='none')\n\nLST_map = LST_map.options(xlabel=\"Longitude\", ylabel=\"Latitude\")\n\nLST_map\n\n\nNDVI_clip = NDVI.rio.clip(landcover_latlon.geometry.values,landcover_latlon.crs, all_touched=True)\nNDVI_map = NDVI_clip.hvplot.image(\n    cmap='RdYlGn',\n    alpha=FIG_ALPHA,\n    width=FIG_WIDTH_PX,\n    height=FIG_HEIGHT_PX,\n    title = \"Carpinteria Salt Marsh Vegetation Index\"\n) * landcover_latlon.hvplot(fill_color='none')\n\nNDVI_map = NDVI_map.options(xlabel=\"Longitude\", ylabel=\"Latitude\")\n\nNDVI_map\n\nHere’s another way we can visualize them, we can map them side by side laid over a satellite basemap, also setting the alpha to a lower value to increase transparency of the raster datasets\n\nLSTmap1 = LST_clip.hvplot.image(\n    tiles=BASEMAP,\n    cmap='jet',\n    alpha=.6,\n    title = \"Carpinteria Salt Marsh Surface Temperature (Celsius)\"\n)\n\nNDVImap1 = NDVI_clip.hvplot.image(\n    tiles=BASEMAP,\n    cmap='RdYlGn',\n    alpha=.6,\n    title = \"Carpinteria Salt Marsh Vegetation Index\"\n)\n\nLSTmap1.options(xlabel=\"Longitude\", ylabel=\"Latitude\")+NDVImap1.options(xlabel=\"Longitude\", ylabel=\"Latitude\")"
  },
  {
    "objectID": "index.html#description",
    "href": "index.html#description",
    "title": "Space Station Synergies: Applying ECOSTRESS and EMIT ecological problems for Scientific Insight",
    "section": "Description:",
    "text": "Description:\nThe International Space Station is a critical asset for the Earth science community – both for advancing critical science and applications priorities, and as a platform for technology demonstrations/pathfinders. These benefits have been particularly significant in recent years, with the installation and operation of instruments such as ECOSTRESS, a multispectral thermal instrument, and EMIT, a visible to short wave infrared imaging spectrometer with best-in-class signal to noise - both acquiring data at field-scale (&lt;70-m). With both sensors mounted on the ISS, there is an unprecedented opportunity to demonstrate the compounded benefits of working with both datasets. In this workshop we highlight the power of these tools when used together, through the use of open source tools and services, cloud compute resources to effectively combine data from ECOSTRESS and EMIT to perform scientific analyses and apply data to real world issues."
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "Space Station Synergies: Applying ECOSTRESS and EMIT ecological problems for Scientific Insight",
    "section": "Learning Outcomes:",
    "text": "Learning Outcomes:\nImaging Spectroscopy and thermal measurements 101, the electromagnetic spectrum and sensor specific considerations How to access EMIT and ECOSTRESS data Data Preprocessing and Exploratory Analysis How to manipulate, combine, and visualize EMIT and ECOSTRESS data Participants will learn the basics of thermal and visible to shortwave infrared (VSWIR) imaging spectroscopy, including how the measurements are different and why they complement each other. Participants will learn how to find and access both EMIT and ECOSTRESS data through available tools and open data use resources managed by the LP DAAC using a cloud environment accessed with their own computer. We will provide examples of integrated analysis that have relevance to applied sciences. Tutorial examples may include vignettes related to agriculture, aquatic heat waves and algal blooms, methane emissions and flaring, and forest health, wildfire risk and stress."
  },
  {
    "objectID": "index.html#learning-focus",
    "href": "index.html#learning-focus",
    "title": "Space Station Synergies: Applying ECOSTRESS and EMIT ecological problems for Scientific Insight",
    "section": "Learning Focus:",
    "text": "Learning Focus:\nPractical Skills for Science"
  },
  {
    "objectID": "index.html#knowledge-career-level",
    "href": "index.html#knowledge-career-level",
    "title": "Space Station Synergies: Applying ECOSTRESS and EMIT ecological problems for Scientific Insight",
    "section": "Knowledge & Career Level:",
    "text": "Knowledge & Career Level:\nBeginner, Intermediate"
  },
  {
    "objectID": "index.html#target-audience",
    "href": "index.html#target-audience",
    "title": "Space Station Synergies: Applying ECOSTRESS and EMIT ecological problems for Scientific Insight",
    "section": "Target Audience:",
    "text": "Target Audience:\n\nEarth and Planetary Surface Processes\nHydrology\nGlobal Environmental Change\nOcean Sciences\nScience and Society\nBiogeosciences"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Contributor Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.\n\n\n\nExamples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\n\n\n\nCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.\n\n\n\nThis Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.\n\n\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at LPDAAC@usgs.gov. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.\n\n\n\nCommunity leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.\n\n\n\n\nThis Code of Conduct is adapted from the [Zarr Developers][Github], available at [https://github.com/zarr-developers/.github/blob/main/CODE_OF_CONDUCT.md] and from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla’s code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-pledge",
    "href": "CODE_OF_CONDUCT.html#our-pledge",
    "title": "Contributor Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-standards",
    "href": "CODE_OF_CONDUCT.html#our-standards",
    "title": "Contributor Code of Conduct",
    "section": "",
    "text": "Examples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "href": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "title": "Contributor Code of Conduct",
    "section": "",
    "text": "Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#scope",
    "href": "CODE_OF_CONDUCT.html#scope",
    "title": "Contributor Code of Conduct",
    "section": "",
    "text": "This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement",
    "href": "CODE_OF_CONDUCT.html#enforcement",
    "title": "Contributor Code of Conduct",
    "section": "",
    "text": "Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at LPDAAC@usgs.gov. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "href": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "title": "Contributor Code of Conduct",
    "section": "",
    "text": "Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#attribution",
    "href": "CODE_OF_CONDUCT.html#attribution",
    "title": "Contributor Code of Conduct",
    "section": "",
    "text": "This Code of Conduct is adapted from the [Zarr Developers][Github], available at [https://github.com/zarr-developers/.github/blob/main/CODE_OF_CONDUCT.md] and from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla’s code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations."
  },
  {
    "objectID": "CHANGE_LOG.html",
    "href": "CHANGE_LOG.html",
    "title": "Change Log",
    "section": "",
    "text": "All notable changes to this project will be documented in this file. The format is based on Keep a Changelog and this project adheres to Semantic Versioning. _________________________________________________________________________\n\n\n\n\n\nImproved Finding Concurrent Data Notebook text/instructions\nRenamed contribute.md\nadded repo description\n\n\n\nRepository description\n\n\n\n\n\n\n\nUpdated notebook ROI to Carpenteria Salt Marsh\n\n\nAdded landcover.geojson\n\n\n\n\n\n\n\nUpdated contribute.md and added user contributed directory\n\n\nAdded user_contributed directory\n\n\n\n\n\n\n\nThis is the first update.\n\n\nFinding Concurrent Data Notebook"
  },
  {
    "objectID": "CHANGE_LOG.html#section",
    "href": "CHANGE_LOG.html#section",
    "title": "Change Log",
    "section": "",
    "text": "Improved Finding Concurrent Data Notebook text/instructions\nRenamed contribute.md\nadded repo description\n\n\n\nRepository description"
  },
  {
    "objectID": "CHANGE_LOG.html#section-1",
    "href": "CHANGE_LOG.html#section-1",
    "title": "Change Log",
    "section": "",
    "text": "Updated notebook ROI to Carpenteria Salt Marsh\n\n\nAdded landcover.geojson"
  },
  {
    "objectID": "CHANGE_LOG.html#section-2",
    "href": "CHANGE_LOG.html#section-2",
    "title": "Change Log",
    "section": "",
    "text": "Updated contribute.md and added user contributed directory\n\n\nAdded user_contributed directory"
  },
  {
    "objectID": "CHANGE_LOG.html#section-3",
    "href": "CHANGE_LOG.html#section-3",
    "title": "Change Log",
    "section": "",
    "text": "This is the first update.\n\n\nFinding Concurrent Data Notebook"
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "Contributing to this Repository",
    "section": "",
    "text": "Please submit a pull request early in the development phase, outlining the changes you intend to make or features you intend to add. This allows us to offer feedback early on, ensuring your contribution can be added to the repository before you invest a significant amount of time.\n\nWe want your help! Even if you’re not a coder! There are several ways you can contribute to this repository:\n\nReport an Issue or make a recommendation\nUpdate code, documentation, notebooks or other files (even fixing typos)\nPropose a new notebook\n\nIn the sections below we outline how to approach each of these types of contributions. If you’re new to GitHub, you can sign up here. There are a bunch of great resources on the GitHub Quickstart page. The GitHub Cheatsheet is also quite helpful, even for experienced users. Please reach out to lpdaac@usgs.gov with questions or concerns.\n\n\nIf you’ve found a problem with the repository, we want to know about it! Please submit an Issue. Before submitting, we would appreciate if you check to see if a similar issue already exists. If not, create a new issue, providing as much detail as possible. Things like screenshots and code excerpts demonstrating the problem are very helpful!\n\n\n\nTo contribute a solution to an issue or make a change to files within the repository we’ve created a typical outline of how to do that below. If you want to make a simple change, like correcting a typo within a markdown document or other documentation, there’s a great video explaining how to do that without leaving the GitHub website here. To make a more complex change to a notebook, code, or other file follow the instructions below.\n\nPlease create an Issue or comment on an existing issue describing the changes you intend to make.\n\nCreate a fork of this repository. This will create your own copy of the repository. When working from your fork, you can do whatever you want, you won’t mess up anyone else’s work so you’re safe to try things out. Worst case scenario you can delete your fork and recreate it.\n\nClone your fork to your local computer or cloud workspace using your preferred command line interface after navigating to the directory you want to place the repository in:\ngit clone your-fork-repository-url\n\nChange directories to the one you cloned\n\ncd repository-name\n\nAdd the upstream repository, this is the original repository that you want to contribute to.\n\ngit remote add upstream original-repository-url\n\nYou can use the following to view the remote repositories:\n\ngit remote -v\n\nupstream, which refers to the original repository\n\norigin, which refers to your personal fork\n\nDevelop your contribution:\n\nCreate a new branch named appropriately for the feature you want to work on:\n\ngit checkout -b new-branch-name\n\nOften, updates to an upstream repository will occur while you are developing changes on your personal fork. You can pull the latest changes from upstream\n\ngit pull upstream dev\n\nYou can check the status of your local copy of the repository to see what changes have been made using:\n\ngit status\n\nCommit locally as you progress using git add and git commit.` For example, updating a readme.md file:\n\ngit add readme.md\ngit commit -m \"updated readme file\"\n\nYou can check the status of your local copy of the repository again to see what pending changes have not been added or committed using:\n\ngit status\n\nAfter making some changes, push your changes back to your fork on GitHub:\n\ngit push origin branch-name\n\nEnter username and password, depending on your settings, you may need to use a Personal access token\n\nTo submit your contribution, navigate to your forked repository GitHub page and make a pull request using the Compare &pull request green button. Make sure to select the base repository and its dev branch. Also select your forked repository as head repository and make sure compare shows your branch name. You can add your comments and press Create pull request green button. Our team will be notified and will review your suggested revisions.\n\nPlease submit a pull request early in the development phase, outlining the changes you intend to make or features you intend to add. This allows us to offer feedback early on, ensuring your contribution can be added to the repository before you invest a significant amount of time.\n\n\n\n\n\nIn the spirit of open science, we want to minimize barriers to sharing code and examples. We have added a user_contributed directory to the repository for anyone to share examples of their work in notebook or code form. Documentation and descriptions do not need to be as thorough as the examples we’ve created, but we ask that you provide as much as possible. Follow the instructions above, placing your new notebook or module in a suitably named directory within the user_contributed directory. Be sure to remove any large datasets and indicate where users can retrieve them.\n\n\n\nThese contributing guidelines are adapted from the NASA Transform to Open Science GitHub, available at https://github.com/nasa/Transform-to-Open-Science/blob/main/CONTRIBUTING.md."
  },
  {
    "objectID": "CONTRIBUTING.html#report-an-issue-or-make-a-recommendation",
    "href": "CONTRIBUTING.html#report-an-issue-or-make-a-recommendation",
    "title": "Contributing to this Repository",
    "section": "",
    "text": "If you’ve found a problem with the repository, we want to know about it! Please submit an Issue. Before submitting, we would appreciate if you check to see if a similar issue already exists. If not, create a new issue, providing as much detail as possible. Things like screenshots and code excerpts demonstrating the problem are very helpful!"
  },
  {
    "objectID": "CONTRIBUTING.html#updating-code-documentation-notebooks-or-other-files",
    "href": "CONTRIBUTING.html#updating-code-documentation-notebooks-or-other-files",
    "title": "Contributing to this Repository",
    "section": "",
    "text": "To contribute a solution to an issue or make a change to files within the repository we’ve created a typical outline of how to do that below. If you want to make a simple change, like correcting a typo within a markdown document or other documentation, there’s a great video explaining how to do that without leaving the GitHub website here. To make a more complex change to a notebook, code, or other file follow the instructions below.\n\nPlease create an Issue or comment on an existing issue describing the changes you intend to make.\n\nCreate a fork of this repository. This will create your own copy of the repository. When working from your fork, you can do whatever you want, you won’t mess up anyone else’s work so you’re safe to try things out. Worst case scenario you can delete your fork and recreate it.\n\nClone your fork to your local computer or cloud workspace using your preferred command line interface after navigating to the directory you want to place the repository in:\ngit clone your-fork-repository-url\n\nChange directories to the one you cloned\n\ncd repository-name\n\nAdd the upstream repository, this is the original repository that you want to contribute to.\n\ngit remote add upstream original-repository-url\n\nYou can use the following to view the remote repositories:\n\ngit remote -v\n\nupstream, which refers to the original repository\n\norigin, which refers to your personal fork\n\nDevelop your contribution:\n\nCreate a new branch named appropriately for the feature you want to work on:\n\ngit checkout -b new-branch-name\n\nOften, updates to an upstream repository will occur while you are developing changes on your personal fork. You can pull the latest changes from upstream\n\ngit pull upstream dev\n\nYou can check the status of your local copy of the repository to see what changes have been made using:\n\ngit status\n\nCommit locally as you progress using git add and git commit.` For example, updating a readme.md file:\n\ngit add readme.md\ngit commit -m \"updated readme file\"\n\nYou can check the status of your local copy of the repository again to see what pending changes have not been added or committed using:\n\ngit status\n\nAfter making some changes, push your changes back to your fork on GitHub:\n\ngit push origin branch-name\n\nEnter username and password, depending on your settings, you may need to use a Personal access token\n\nTo submit your contribution, navigate to your forked repository GitHub page and make a pull request using the Compare &pull request green button. Make sure to select the base repository and its dev branch. Also select your forked repository as head repository and make sure compare shows your branch name. You can add your comments and press Create pull request green button. Our team will be notified and will review your suggested revisions.\n\nPlease submit a pull request early in the development phase, outlining the changes you intend to make or features you intend to add. This allows us to offer feedback early on, ensuring your contribution can be added to the repository before you invest a significant amount of time."
  },
  {
    "objectID": "CONTRIBUTING.html#adding-new-notebooks-or-example-workflows",
    "href": "CONTRIBUTING.html#adding-new-notebooks-or-example-workflows",
    "title": "Contributing to this Repository",
    "section": "",
    "text": "In the spirit of open science, we want to minimize barriers to sharing code and examples. We have added a user_contributed directory to the repository for anyone to share examples of their work in notebook or code form. Documentation and descriptions do not need to be as thorough as the examples we’ve created, but we ask that you provide as much as possible. Follow the instructions above, placing your new notebook or module in a suitably named directory within the user_contributed directory. Be sure to remove any large datasets and indicate where users can retrieve them."
  },
  {
    "objectID": "CONTRIBUTING.html#attribution",
    "href": "CONTRIBUTING.html#attribution",
    "title": "Contributing to this Repository",
    "section": "",
    "text": "These contributing guidelines are adapted from the NASA Transform to Open Science GitHub, available at https://github.com/nasa/Transform-to-Open-Science/blob/main/CONTRIBUTING.md."
  },
  {
    "objectID": "python/01_Finding_Concurrent_Data.html",
    "href": "python/01_Finding_Concurrent_Data.html",
    "title": "01 Finding Concurrent ECOSTRESS and EMIT Data",
    "section": "",
    "text": "If running this notebook locally, you will find instructions to set up a compatible environment in the setup folder. If running on the Openscapes 2i2c Cloud Instance for a Workshop, no additional setup is required.\nSummary\nBoth the ECOsystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS) and the Earth surface Mineral dust source InvesTigation (EMIT) instruments are located on the International Space Station (ISS). Their overlapping fields of view provide an unprecedented opportunity to demonstrate the compounded benefits of working with both datasets. In this notebook we will show how to utilize the earthaccess Python library to find concurrent ECOSTRESS and EMIT data.\nBackground\nThe ECOSTRESS insturment is a multispectral thermal imaging radiometer designed to answer three overarching science questions:\nThe ECOSTRESS mission is answering these questions by accurately measuring the temperature of plants. Plants regulate their temperature by releasing water through tiny pores on their leaves called stomata. If they have sufficient water they can maintain their temperature, but if there is insufficient water, their temperatures rise and this temperature rise can be measured with ECOSTRESS. The images acquired by ECOSTRESS are the most detailed temperature images of the surface ever acquired from space and can be used to measure the temperature of an individual farmers field.\nMore details about ECOSTRESS and its associated products can be found on the ECOSTRESS website and ECOSTRESS product pages hosted by the Land Processes Distributed Active Archive Center (LP DAAC).\nThe EMIT instrument is an imaging spectrometer that measures light in visible and infrared wavelengths. These measurements display unique spectral signatures that correspond to the composition on the Earth’s surface. The EMIT mission focuses specifically on mapping the composition of minerals to better understand the effects of mineral dust throughout the Earth system and human populations now and in the future. In addition, the EMIT instrument can be used in other applications, such as mapping of greenhouse gases, snow properties, and water resources.\nMore details about EMIT and its associated products can be found on the EMIT website and EMIT product pages hosted by the LP DAAC.\nRequirements - NASA Earthdata Account - No Python setup requirements if connected to the workshop cloud instance! - Set up Python Environment - See setup_instructions.md in the /setup/ folder\nLearning Objectives\n- How to use earthaccess to find concurrent EMIT and ECOSTRESS data. - How to export a list of files and download them programatically.\nTutorial Outline"
  },
  {
    "objectID": "python/01_Finding_Concurrent_Data.html#setup",
    "href": "python/01_Finding_Concurrent_Data.html#setup",
    "title": "01 Finding Concurrent ECOSTRESS and EMIT Data",
    "section": "1. Setup",
    "text": "1. Setup\nImport the required Python libraries.\n\n# Import required libraries\nimport os\nimport folium\nimport earthaccess\nimport warnings\nimport folium.plugins\nimport pandas as pd\nimport geopandas as gpd\nimport math\n\nfrom branca.element import Figure\nfrom IPython.display import display\nfrom shapely import geometry\nfrom skimage import io\nfrom datetime import timedelta\nfrom shapely.geometry.polygon import orient\nfrom matplotlib import pyplot as plt\n\n\n1.2 NASA Earthdata Login Credentials\nTo download or stream NASA data you will need an Earthdata account, you can create one here. We will use the login function from the earthaccess library for authentication before downloading at the end of the notebook. This function can also be used to create a local .netrc file if it doesn’t exist, or add your login info to an existing .netrc file. If no Earthdata Login credentials are found in the .netrc you’ll be prompted for them. This step is not necessary to conduct searches, but is needed to download or stream data."
  },
  {
    "objectID": "python/01_Finding_Concurrent_Data.html#search-for-ecostress-and-emit-data",
    "href": "python/01_Finding_Concurrent_Data.html#search-for-ecostress-and-emit-data",
    "title": "01 Finding Concurrent ECOSTRESS and EMIT Data",
    "section": "2. Search for ECOSTRESS and EMIT Data",
    "text": "2. Search for ECOSTRESS and EMIT Data\nBoth EMIT and ECOSTRESS products are hosted by the Land Processes Distributed Active Archive Center (LP DAAC). In this example we will use the cloud-hosted EMIT_L2A_RFL and ECOSTRESS_L2T_LSTE products available from the LP DAAC to find data. Any results we find for these products, should be available for other products within the EMIT and ECOSTRESS collections.\nTo find data we will use the earthaccess Python library. earthaccess searches NASA’s Common Metadata Repository (CMR), a metadata system that catalogs Earth Science data and associated metadata records. The results can then be used to download granules or generate lists granule search result URLs.\nUsing earthaccess we can search based on the attributes of a granule, which can be thought of as a spatiotemporal scene from an instrument containing multiple assets (eg. Reflectance, Reflectance Uncertainty, Masks for the EMIT L2A Reflectance Collection). We can search using attributes such as collection, acquisition time, and spatial footprint. This process can also be used with other EMIT or ECOSTRESS products, other collections, or different data providers, as well as across multiple catalogs with some modification.\n\n2.1 Define Spatial Region of Interest\nFor this example, our spatial region of interest (ROI) will be the Carpenteria Salt Marsh. You can learn more about it here: https://ucnrs.org/reserves/carpinteria-salt-marsh-reserve/. If you want to create a geojson polygon for your own ROI, you can do so using this website: https://geojson.io/#map=2/20/0, or you can convert a shapefile to a geojson using some code in the Appendices.\nIn this example, we elect to search using a polygon rather than a standard bounding box because bounding boxes will have a larger spatial extent, capturing a lot of area we may not be interested in. This becomes more important for searches with larger ROIs than our example here. To search for intersections with a polygon using earthaccess, we need to format our ROI as a counter-clockwise list of coordinate pairs.\nOpen the geojson file containing a landcover classification of Carpenteria Salt Marsh as a geodataframe, and check the coordinate reference system (CRS) of the data.\n\npolygon = gpd.read_file('../data/landcover.geojson')\npolygon.crs\n\nThe CRS is EPSG:4326 (WGS84), which is also the CRS we want the data in to submit for our search.\nNext, lets examine our polygon a bit closer.\n\npolygon\n\nWe can see this geodataframe consists of multiple classes, each containing a multipolygon within our study site. We need to create an exterior boundary polygon containing these, and make sure the vertices are in counter-clockwise order to submit them in our query. To do this, create a polygon consisting of all the geometries, then calculate the convex hull of the union. This will give us a simple exterior polygon around our full ROI. After that, use the orient function to place our coordinate pairs in counter-clockwise order.\n\n# Merge all Polygon geometries and create external boundary\nroi_poly = polygon.unary_union.convex_hull\n# Re-order vertices to counter-clockwise\nroi_poly = orient(roi_poly, sign=1.0)\n\nWe can go ahead and visualize our region of interest and the original landcover polygon. First add a function to help reformat bound box coordinates to work with leaflet notation.\n\n# Function to convert a bounding box for use in leaflet notation\n\ndef convert_bounds(bbox, invert_y=False):\n    \"\"\"\n    Helper method for changing bounding box representation to leaflet notation\n\n    ``(lon1, lat1, lon2, lat2) -&gt; ((lat1, lon1), (lat2, lon2))``\n    \"\"\"\n    x1, y1, x2, y2 = bbox\n    if invert_y:\n        y1, y2 = y2, y1\n    return ((y1, x1), (y2, x2))\n\nThen create a figure using folium.\n\nfig = Figure(width=\"800px\", height=\"400px\")\nmap1 = folium.Map(tiles='https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}', attr='Google')\nfig.add_child(map1)\n\n# Add Convex Hull Polygon\nfolium.GeoJson(roi_poly,\n                name='convex hull',\n                ).add_to(map1)\n\n# Add landcover classification geodataframe\npolygon.explore(\n    \"type\",\n    popup=True,\n    categorical=True,\n    cmap='Set3',\n    style_kwds=dict(opacity=0.7, fillOpacity=0.4),\n    name=\"Carpenteria Salt Marsh Landcover\",\n    m=map1\n)\n\nmap1.add_child(folium.LayerControl())\nmap1.fit_bounds(bounds=convert_bounds(polygon.unary_union.bounds))\ndisplay(fig)\n\nAbove we can see our region of interest (ROI) and the landcover classification polygon that we opened. We can hover over different areas to see the land cover class.\nLastly we need to convert our polygon to a list of coordinate pairs.\n\n# Set ROI as list of exterior polygon vertices as coordinate pairs\nroi = list(roi_poly.exterior.coords)\n\n\n\n2.2 Define Collections of Interest\nWe need to specify which products we want to search for using their short-names. As mentioned above, we will conduct our search using the EMIT Level 2A Reflectance (EMITL2ARFL) and ECOSTRESS Level 2 Tiled Land Surface Temperature and Emmissivity (ECO_L2T_LSTE).\n\nNote: Here we use the Tiled ECOSTRESS LSTE Product. This will also work with the gridded LSTE and the swath; however, the swath product does not have a browse image for the visualization in section 4, and will require additional processing for subsequent analysis.\n\n\n# Data Collections for our search\ncollections = ['EMITL2ARFL', 'ECO_L2T_LSTE']\n\n\n\n2.3 Define Date Range\nFor our date range, we’ll look at data collected across the month of April 2023. The date_range can be specified as a pair of dates, start and end (up to, not including).\n\n# Define Date Range\ndate_range = ('2023-01-01','2023-09-01')\n\n\n\n2.4 Searching\nSubmit a query using earthaccess.\n\nresults = earthaccess.search_data(\n    short_name=collections,\n    polygon=roi,\n    temporal=date_range,\n    count=500\n)\n\n\nlen(results)"
  },
  {
    "objectID": "python/01_Finding_Concurrent_Data.html#organizing-and-filtering-results",
    "href": "python/01_Finding_Concurrent_Data.html#organizing-and-filtering-results",
    "title": "01 Finding Concurrent ECOSTRESS and EMIT Data",
    "section": "3. Organizing and Filtering Results",
    "text": "3. Organizing and Filtering Results\nAs we can see from above, the results object contains a list of objects with metadata and links. We can convert this to a more readable format, a dataframe. In addition, we can make it a geodataframe by taking the spatial metadata and creating a shapely polygon representing the spatial coverage, and further customize which information we want to use from other metadata fields.\nFirst, we define some functions to help us create a shapely object for our geodataframe, and retrieve the specific browse image URLs that we want. By default the browse image selected by earthaccess is the first one in the list, but the ECO_L2_LSTE has several browse images and we want to make sure we retrieve the png file, which is a preview of the LSTE.\n\n# Function to create shapely polygon of spatial coverage\ndef get_shapely_object(result:earthaccess.results.DataGranule):\n    # Get Geometry Keys\n    geo = result['umm']['SpatialExtent']['HorizontalSpatialDomain']['Geometry']\n    keys = geo.keys()\n\n    if 'BoundingRectangles' in keys:\n        bounding_rectangle = geo['BoundingRectangles'][0]\n        # Create bbox tuple\n        bbox_coords = (bounding_rectangle['WestBoundingCoordinate'],bounding_rectangle['SouthBoundingCoordinate'],\n                    bounding_rectangle['EastBoundingCoordinate'],bounding_rectangle['NorthBoundingCoordinate'])\n        # Create shapely geometry from bbox\n        shape = geometry.box(*bbox_coords, ccw=True)\n    elif 'GPolygons' in keys:\n        points = geo['GPolygons'][0]['Boundary']['Points']\n        # Create shapely geometry from polygons\n        shape = geometry.Polygon([[p['Longitude'],p['Latitude']] for p in points])\n    else:\n         raise ValueError('Provided result does not contain bounding boxes/polygons or is incompatible.')\n    return(shape)\n\n# Retrieve png browse image if it exists or first jpg in list of urls\ndef get_png(result:earthaccess.results.DataGranule):\n    https_links = [link for link in result.dataviz_links() if 'https' in link]\n    if len(https_links) == 1:\n        browse = https_links[0]\n    elif len(https_links) == 0:\n        browse = 'no browse image'\n        warnings.warn(f\"There is no browse imagery for {result['umm']['GranuleUR']}.\")\n    else:\n        browse = [png for png in https_links if '.png' in png][0]\n    return(browse)\n\nNow that we have our functions we can create a dataframe, then calculate and add our shapely geometries to make a geodataframe. After that, add a column for our browse image urls and print the number of granules in our results, so we can monitor the quantity we are working with a we winnow down to the data we want.\n\n# Create Dataframe of Results Metadata\nresults_df = pd.json_normalize(results)\n# Create shapely polygons for result\ngeometries = [get_shapely_object(results[index]) for index in results_df.index.to_list()]\n# Convert to GeoDataframe\ngdf = gpd.GeoDataFrame(results_df, geometry=geometries, crs=\"EPSG:4326\")\n# Remove results df, no longer needed\ndel results_df\n# Add browse imagery links\ngdf['browse'] = [get_png(granule) for granule in results]\ngdf['shortname'] = [result['umm']['CollectionReference']['ShortName'] for result in results]\n# Preview GeoDataframe\nprint(f'{gdf.shape[0]} granules total')\n\nPreview our geodataframe to get an idea what it looks like.\n\ngdf.head()\n\nThere are a lot of columns with data that is not relevant to our goal, so we can drop those. To do that, list the names of colums.\n\n# List Column Names\ngdf.columns\n\nNow create a list of columns to keep and use it to filter the dataframe.\n\n# Create a list of columns to keep\nkeep_cols = ['meta.concept-id','meta.native-id', 'umm.TemporalExtent.RangeDateTime.BeginningDateTime','umm.TemporalExtent.RangeDateTime.EndingDateTime','umm.CloudCover','umm.DataGranule.DayNightFlag','geometry','browse', 'shortname']\n# Remove unneeded columns\ngdf = gdf[gdf.columns.intersection(keep_cols)]\ngdf.head()\n\nThis is looking better, but we can make it more readable by renaming our columns.\n\n# Rename some Columns\ngdf.rename(columns = {'meta.concept-id':'concept_id','meta.native-id':'granule',\n                       'umm.TemporalExtent.RangeDateTime.BeginningDateTime':'start_datetime',\n                      'umm.TemporalExtent.RangeDateTime.EndingDateTime':'end_datetime',\n                      'umm.CloudCover':'cloud_cover',\n                      'umm.DataGranule.DayNightFlag':'day_night'}, inplace=True)\ngdf.head()\n\n\nNote: If querying on-premises (not cloud) LP DAAC datasets, the meta.concept-id will not show as xxxxxx-LPCLOUD. For these datasets, the granule name can be retrieved from the umm.DataGranule.Identifiers column.\n\nWe can filter using the day/night flag as well, but this step will be unnecessary as we check to ensure all results from ECOSTRESS fall within an hour of resulting EMIT granules.\n\n# gdf = gdf[gdf['day_night'].str.contains('Day')]\n\nOur first step toward filtering the datasets will be to add a column with a datetime.\n\nYou may have noticed that the date format is similar for ECOSTRESS and EMIT, but the ECOSTRESS data has an additional fractional seconds. If using the recommended lpdaac_vitals Windows environment, you will need to pass the format='ISO8601'argument to the to_datetime function, like in the commented out line.\n\n\ngdf['datetime_obj'] = pd.to_datetime(gdf['start_datetime'])\n# gdf['datetime_obj'] = pd.to_datetime(gdf['start_datetime'], format='ISO8601')\n\nWe can roughly visualize the quantity of results by month at our location using a histogram with 8 bins (Jan up to Sept).\n\ngdf.hist(column='datetime_obj', by='shortname', bins=9, color='green', edgecolor='black', linewidth=1, sharey=True)\n\nNow we will separate the results into two dataframes, one for ECOTRESS and one for EMIT and print the number of results for each so we can monitor how many granules we’re filtering.\n\n# Suppress Setting with Copy Warning - not applicable in this use case\npd.options.mode.chained_assignment = None  # default='warn'\n\n# Split into two dataframes - ECO and EMIT\neco_gdf = gdf[gdf['granule'].str.contains('ECO')]\nemit_gdf = gdf[gdf['granule'].str.contains('EMIT')]\nprint(f' ECOSTRESS Granules: {eco_gdf.shape[0]} \\n EMIT Granules: {emit_gdf.shape[0]}')\n\n\nemit_gdf.head()\n\nWe still haven’t filtered the locations where EMIT and ECOSTRESS have data at the same spatial location and time-frame. The EMIT acquisition mask has been added to ECOSTRESS, so in most cases if EMIT is collecting data, so will ECOSTRESS, but there are edge cases where this is not true. To do this we’ll use two filters to catch the edge-cases, and provide an example that can be used with other datasets.\nFirst, since EMIT has a smaller swath width, we can can use a unary union of the spatial coverage present in our geodataframe to filter out ecostress granules that do not overlap with it.\n\n# Subset ECOSTRESS Granules in Geodataframe by intersection with EMIT granules\n## Create new column based on intersection with union of EMIT polygons.\neco_gdf['intersects'] = eco_gdf.intersects(emit_gdf.unary_union)\n## Apply subsetting\neco_gdf = eco_gdf[eco_gdf['intersects'] == True]\nprint(f' ECOSTRESS Granules: {eco_gdf.shape[0]} \\n EMIT Granules: {emit_gdf.shape[0]}')\n\nIn this instance, our results aren’t narrowed because our region of interest is smaller than a single EMIT scene. If the spatial ROI was very large, this would be much more unlikely.\nAdditionally, we want to make sure that data in our results are collected at the same time. For EMIT and ECOSTRESS, the EMIT acquisition mask has been added to the ECOSTRESS mask, meaning that if there is an EMIT scene, there should also be an ECOSTRESS scene acquired at the same time. In practice, however, the timestamps on the scenes can vary slightly. In order to capture this slight variability, we need to use a range instead of a single timestamp to capture concurrent data. To do this, we’ll ensure all ECOSTRESS granule start times fall within 10 minutes of any of the EMIT granules in our results, and vice-versa.\nWrite a function to evaluate whether these datetime objects fall within 10 minutes of one another using the timedelta function.\n\n# Function to Filter timestamps that do not fall within a time_delta of timestamps from the other acquisition time\ndef concurrent_match(gdf_a:pd.DataFrame, gdf_b:pd.DataFrame, col_name:str, time_delta:timedelta):\n    \"\"\"\n    Cross references dataframes containing a datetime object column and keeps rows in \n    each that fall within the provided timedelta of the other. Acceptable time_delta examples:\n    \n    months=1\n    days=1\n    hours=1\n    minutes=1\n    seconds=1 \n\n    \"\"\"\n    # Match Timestamps from Dataframe A with Time-range of entries in Dataframe B\n    # Create empty list\n    a_list = []\n    # Iterate results for product a based on index values\n    for _n in gdf_b.index.to_list():\n        # Find where product b is within the window of each product a result\n        a_matches = (gdf_a[col_name] &gt; gdf_b[col_name][_n]-time_delta) & (gdf_a[col_name] &lt; gdf_b[col_name][_n]+time_delta)\n        # Append list with values\n        a_list.append(a_matches)\n    # Match Timestamps from Dataframe B with Time-range of entries in Dataframe A\n    # Create empy list\n    b_list =[]\n    for _m in gdf_a.index.to_list():\n        # Find where product a is within the window of each product b result\n        b_matches = (gdf_b[col_name] &gt; gdf_a[col_name][_m]-time_delta) &  (gdf_b[col_name] &lt; gdf_a[col_name][_m]+time_delta)\n        # Append list with values\n        b_list.append(b_matches)\n    # Filter Original Dataframes by summing list of bools, 0 = outside of all time-ranges\n    a_filtered = gdf_a.loc[sum(a_list) &gt; 0]\n    b_filtered = gdf_b.loc[sum(b_list) &gt; 0]\n    return(a_filtered, b_filtered)\n\nNow run our function.\n\n\neco_gdf, emit_gdf = concurrent_match(eco_gdf,emit_gdf, col_name='datetime_obj',time_delta=timedelta(minutes=10))\nprint(f' ECOSTRESS Granules: {eco_gdf.shape[0]} \\n EMIT Granules: {emit_gdf.shape[0]}')"
  },
  {
    "objectID": "python/01_Finding_Concurrent_Data.html#visualizing-intersecting-coverage",
    "href": "python/01_Finding_Concurrent_Data.html#visualizing-intersecting-coverage",
    "title": "01 Finding Concurrent ECOSTRESS and EMIT Data",
    "section": "4. Visualizing Intersecting Coverage",
    "text": "4. Visualizing Intersecting Coverage\nNow that we have geodataframes containing some concurrent data, we can visualize them on a map using folium. It’s often difficult to visualize a large time-series of scenes, so we’ve included an example in Appendix A1 on how to filter to a single day.\n\n# Plot Using Folium\n\n# Create Figure and Select Background Tiles\nfig = Figure(width=\"1100px\", height=\"550px\")\nmap1 = folium.Map(tiles='https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}', attr='Google')\nfig.add_child(map1)\n\n# Plot STAC ECOSTRESS Results - note we must drop the datetime_obj columns for this to work\neco_gdf.drop(columns=['datetime_obj']).explore(\n    \"granule\",\n    categorical=True,\n    tooltip=[\n        \"granule\",\n        \"start_datetime\",\n        \"cloud_cover\",\n    ],\n    popup=True,\n    style_kwds=dict(fillOpacity=0.1, width=2),\n    name=\"ECOSTRESS\",\n    m=map1,\n)\n\n# Plot STAC EMITL2ARFL Results - note we must drop the datetime_obj columns for this to work\nemit_gdf.drop(columns=['datetime_obj']).explore(\n    \"granule\",\n    categorical=True,\n    tooltip=[\n        \"granule\",\n        \"start_datetime\",\n        \"cloud_cover\",\n    ],\n    popup=True,\n    style_kwds=dict(fillOpacity=0.1, width=2),\n    name=\"EMIT\",\n    m=map1,\n)\n\n# ECOSTRESS Browse Images - Comment out to remove\nfor _n in eco_gdf.index.to_list():\n    folium.raster_layers.ImageOverlay(\n        image=eco_gdf['browse'][_n],\n        name=eco_gdf['granule'][_n],\n        bounds=[[eco_gdf.bounds['miny'][_n], eco_gdf.bounds['minx'][_n]], [eco_gdf.bounds['maxy'][_n], eco_gdf.bounds['maxx'][_n]]],\n        interactive=False,\n        cross_origin=False,\n        opacity=0.75,\n        zindex=1,\n        ).add_to(map1)\n\n# Plot Region of Interest\npolygon.explore(\n    popup=False,\n    style_kwds=dict(fillOpacity=0.1, width=2),\n    name=\"Region of Interest\",\n    m=map1\n)\n\nmap1.fit_bounds(bounds=convert_bounds(gdf.unary_union.bounds))\nmap1.add_child(folium.LayerControl())\ndisplay(fig)\n\nIn the figure above, you can zoom in and out, click and drag to reposition the legend, and add or remove layers using the layer control in the top right. Notice that since we’re using the tiled ecostress product, we have 2 overlapping tiles at our ROI. You can visualize the tiles by adding or removing the layers.\n\n4.2 Previewing EMIT Browse Imagery\nThe EMIT browse imagery is not orthorectified, so it can’t be visualized on a plot like the ECOSTRESS browse imagery. To get an idea what scenes look like we can plot them in a grid using matplotlib.\n\nNote: The black space is indicative of onboard cloud masking that occurs before data is downlinked from the ISS.\n\n\ncols = 3\nrows = math.ceil(len(emit_gdf)/cols)\nfig, ax = plt.subplots(rows, cols, figsize=(20,20))\nax = ax.flatten()\n\nfor _n, index in enumerate(emit_gdf.index.to_list()):\n    img = io.imread(emit_gdf['browse'][index])\n    ax[_n].imshow(img)\n    ax[_n].set_title(f\"Index: {index} - {emit_gdf['granule'][index]}\")\n    ax[_n].axis('off')\nplt.tight_layout()\nplt.show\n\n\n\n4.3 Further Filtering\nWe can see that some of these granules likely won’t work because of the large amount of cloud cover, we can use a list of these to filter them out. Make a list of indexes to filter out.\n\n# Bad granule list\nbad_granules = [27,74,87]\n\nFilter out the bad granules.\n\nemit_gdf = emit_gdf[~emit_gdf.index.isin(bad_granules)]\n\nNow that we’ve narrowed our EMIT results we can again filter the ecostress granules based on their concurrency with our filtered EMIT granules.\n\neco_gdf, emit_gdf = concurrent_match(eco_gdf,emit_gdf, col_name='datetime_obj',time_delta=timedelta(hours=1))\nprint(f' ECOSTRESS Granules: {eco_gdf.shape[0]} \\n EMIT Granules: {emit_gdf.shape[0]}')"
  },
  {
    "objectID": "python/01_Finding_Concurrent_Data.html#generating-a-list-of-urls-and-downloading-data",
    "href": "python/01_Finding_Concurrent_Data.html#generating-a-list-of-urls-and-downloading-data",
    "title": "01 Finding Concurrent ECOSTRESS and EMIT Data",
    "section": "5. Generating a list of URLs and downloading data",
    "text": "5. Generating a list of URLs and downloading data\nCreating a list of results URLs will include all of these assets, so if we only want a subset we need an additional filter to keep the specific assets we want.\nIf you look back, you can see we kept the same indexing throughout the notebook. This enables us to simply subset the earthaccess results object to retrieve the results we want.\nCreate a list of index values to keep.\n\nkeep_granules = eco_gdf.index.to_list()+emit_gdf.index.to_list()\nkeep_granules.sort()\n\nFilter the results list.\n\nfiltered_results = [result for i, result in enumerate(results) if i in keep_granules]\n\nNow we can download all of the associated assets, or retrieve the URLS and further filter them to specifically what we want.\nFirst, log into Earthdata using the login function from the earthaccess library. The persist=True argument will create a local .netrc file if it doesn’t exist, or add your login info to an existing .netrc file. If no Earthdata Login credentials are found in the .netrc you’ll be prompted for them. As mentioned in section 1.2, this step is not necessary to conduct searches, but is needed to download or stream data.\n\nearthaccess.login(persist=True)\n\nNow we can download all assets using the following cell.\n\n# # Download All Assets for Granules in Filtered Results\n# earthaccess.download(filtered_results, '../data/')\n\nOr we can create a list of URLs and use that to further refine which files we download.\n\n# Retrieve URLS for Assets\nresults_urls = [granule.data_links() for granule in filtered_results]\n\nGranules often have several assets associated with them, for example, ECO_L2T_LSTE has several assets: - Water Mask (water) - Cloud Mask (cloud) - Quality (QC) - Land Surface Temperature (LST) - Land Surface Temperature Error (LST_err) - Wide Band Emissivity (EmisWB) - Height (height)\nThe results list we just generated contains URLs to all of these files. We can further filter our results list using string matching to remove unwanted assets.\nCreate a list of strings and enumerate through our results_url list to filter out unwanted assets.\n\nfiltered_asset_links = []\n# Pick Desired Assets (leave _ on RFL to distinguish from RFLUNC, LST. to distinguish from LST_err)\ndesired_assets = ['RFL_', 'LST.'] # Add more or do individually for reflectance, reflectance uncertainty, or mask\n# Step through each sublist (granule) and filter based on desired assets.\nfor n, granule in enumerate(results_urls):\n    for url in granule: \n        asset_name = url.split('/')[-1]\n        if any(asset in asset_name for asset in desired_assets):\n            filtered_asset_links.append(url)\nfiltered_asset_links\n\nUncomment the cell below (select all, then ctrl+/) and download the data that we’ve filtered.\n\n# # Get requests https Session using Earthdata Login Info\n# fs = earthaccess.get_requests_https_session()\n# # Retrieve granule asset ID from URL (to maintain existing naming convention)\n# for url in filtered_asset_links:\n#     granule_asset_id = url.split('/')[-1]\n#     # Define Local Filepath\n#     fp = f'../data/{granule_asset_id}'\n#     # Download the Granule Asset if it doesn't exist\n#     if not os.path.isfile(fp):\n#         with fs.get(url,stream=True) as src:\n#             with open(fp,'wb') as dst:\n#                 for chunk in src.iter_content(chunk_size=64*1024*1024):\n#                     dst.write(chunk)\n\nCongratulations, now you you have downloaded concurrent data from the ECOSTRESS and EMIT instruments on the ISS."
  },
  {
    "objectID": "python/01_Finding_Concurrent_Data.html#contact-info",
    "href": "python/01_Finding_Concurrent_Data.html#contact-info",
    "title": "01 Finding Concurrent ECOSTRESS and EMIT Data",
    "section": "Contact Info:",
    "text": "Contact Info:\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 10-12-2023\n¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I."
  },
  {
    "objectID": "python/01_Finding_Concurrent_Data.html#appendices",
    "href": "python/01_Finding_Concurrent_Data.html#appendices",
    "title": "01 Finding Concurrent ECOSTRESS and EMIT Data",
    "section": "Appendices",
    "text": "Appendices\nThese contain some extra code that may be useful when performing a similar workflow.\n\nA1. Further Limiting Search for Visualization Purposes\nA large quantity of results may be difficult to understand when mapping with folium. We can create a subset that falls within a single day. First add another column of dates only, then find the unique dates.\n\n# eco_gdf['date'] = eco_gdf['start_datetime'].str.split('T').str[0]\n# emit_gdf['date'] = emit_gdf['start_datetime'].str.split('T').str[0]\n# emit_gdf['date'].unique()\n\nFilter both sets of results using a single date.\n\n# single_day_eco = eco_gdf#[eco_gdf['date'] == '2023-04-23']\n# single_day_emit = emit_gdf#[emit_gdf['date'] == '2023-04-23']\n# print(f' ECOSTRESS Granules: {single_day_eco.shape[0]} \\n EMIT Granules: {single_day_emit.shape[0]}')\n\n\n\nA2. Convert Shapefile to GeoJSON\nWe can convert a shapefile to a geojson using the following cell. Note that we need to reorder the polygon external vertices so we can submit them as a list of points for our search.\n\n# # Use Sedgwick Reserve Shapefile\n# # Open Shapefile\n# polygon = gpd.read_file('../data/Sedgwick_Boundary/Sedgwick_Boundary.shp').to_crs(\"EPSG:4326\")\n# # Reorder vertices into Counter-clockwise order\n# polygon.geometry[0] = orient(polygon.geometry[0], sign=1.0)\n# # Save as a geojson (not necessary)\n# polygon.to_file('../data/sedgwick_boundary_epsg4326.geojson', driver='GeoJSON')"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Space Station Synergies AGU Workshop take place on December 10, 2023.\nThis workshop is hosted by NASA’s Land Processes Distributed Activate Archive LP DAAC and NASA Jet Propulsion Laboratory JPL with support from the NASA Openscapes project.\nHands-on exercises will be executed from a Jupyter Hub on the 2i2c cloud instance. Your GitHub username is used to enable access to the cloud instance during the workshop. Please pass along your Github Username to get access if you have not already."
  },
  {
    "objectID": "schedule.html#workshop-schedule",
    "href": "schedule.html#workshop-schedule",
    "title": "Schedule",
    "section": "Workshop Schedule",
    "text": "Workshop Schedule\n\n\n\n\n\n\n\n\nTime\nDescription\nLeads/Instructors\n\n\n\n\n8-8:30 AM\nIntroduction, overview, learning objectives  ECOSTRESS Overview  EMIT Overview\nDana Chadwick & Madeleine Pascolini-Campbell\n\n\n8:30-9:50 AM\n\n\n\n\n10-10:30 AM\nIntroduction to cloud computing environment\nAaron Friesz & Cole Krehbiel\n\n\n10:30- 11:15 AM\nFinding Concurrent Data  - Find and Download the data used for all of the notebooks\nErik Bolch\n\n\n11:15 - 12PM\nIntroduction to EMIT and ECOSTRESS Data\nErik Bolch\n\n\n12PM\nLunch Break\n\n\n\n1 - 1:15 PM\nCWC/EWT from EMIT Reflectance\nErik Bolch\n\n\n1:30 - 2:45 PM\nUse Case 1: ET and CWC for Fire/Burn Severity\nMarie Johnson\n\n\n3 - 4:15 PM\nET and CWC for Agricultural Application\nClaire Villanueva-Weeks\n\n\n4:30 - 5 PM\nDiscussion, feedback, resources to take home\nAll"
  }
]